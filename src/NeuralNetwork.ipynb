{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coCWqLxU2lMo",
   "metadata": {
    "id": "coCWqLxU2lMo"
   },
   "source": [
    "## Подключаем необходимые модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iCaLPVesuaSm",
   "metadata": {
    "id": "iCaLPVesuaSm"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/lyftzeigen/SemanticSegmentationLesson.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179",
   "metadata": {
    "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import measure\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import dilation, disk\n",
    "from skimage.draw import polygon_perimeter\n",
    "\n",
    "print(f'Tensorflow version {tf.__version__}')\n",
    "print(f'GPU is {\"ON\" if tf.config.list_physical_devices(\"GPU\") else \"OFF\" }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3",
   "metadata": {
    "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3"
   },
   "source": [
    "## Подготовим набор данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be98ae-bc6e-4d49-b205-db5130879caa",
   "metadata": {
    "id": "82be98ae-bc6e-4d49-b205-db5130879caa"
   },
   "outputs": [],
   "source": [
    "CLASSES = 8\n",
    "\n",
    "COLORS = ['black', 'red', 'lime',\n",
    "          'blue', 'orange', 'pink',\n",
    "          'cyan', 'magenta']\n",
    "\n",
    "SAMPLE_SIZE = (256, 256)\n",
    "\n",
    "OUTPUT_SIZE = (1080, 1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500caf45-f2b9-48af-8f91-7d31d44ec266",
   "metadata": {
    "id": "500caf45-f2b9-48af-8f91-7d31d44ec266"
   },
   "outputs": [],
   "source": [
    "def load_images(image, mask):\n",
    "    image = tf.io.read_file(image)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    image = tf.image.resize(image, OUTPUT_SIZE)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    \n",
    "    mask = tf.io.read_file(mask)\n",
    "    mask = tf.io.decode_png(mask)\n",
    "    mask = tf.image.rgb_to_grayscale(mask)\n",
    "    mask = tf.image.resize(mask, OUTPUT_SIZE)\n",
    "    mask = tf.image.convert_image_dtype(mask, tf.float32)\n",
    "    \n",
    "    masks = []\n",
    "    \n",
    "    for i in range(CLASSES):\n",
    "        masks.append(tf.where(tf.equal(mask, float(i)), 1.0, 0.0))\n",
    "    \n",
    "    masks = tf.stack(masks, axis=2)\n",
    "    masks = tf.reshape(masks, OUTPUT_SIZE + (CLASSES,))\n",
    "\n",
    "    return image, masks\n",
    "\n",
    "def augmentate_images(image, masks):   \n",
    "    random_crop = tf.random.uniform((), 0.3, 1)\n",
    "    image = tf.image.central_crop(image, random_crop)\n",
    "    masks = tf.image.central_crop(masks, random_crop)\n",
    "    \n",
    "    random_flip = tf.random.uniform((), 0, 1)    \n",
    "    if random_flip >= 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        masks = tf.image.flip_left_right(masks)\n",
    "    \n",
    "    image = tf.image.resize(image, SAMPLE_SIZE)\n",
    "    masks = tf.image.resize(masks, SAMPLE_SIZE)\n",
    "    \n",
    "    return image, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ac3f3-17b7-4fe5-9622-c7d82fbdaff2",
   "metadata": {
    "id": "1d6ac3f3-17b7-4fe5-9622-c7d82fbdaff2"
   },
   "outputs": [],
   "source": [
    "images = sorted(glob.glob('SemanticSegmentationLesson/dataset/images/*.jpg'))\n",
    "masks = sorted(glob.glob('SemanticSegmentationLesson/dataset/masks/*.png'))\n",
    "\n",
    "images_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "masks_dataset = tf.data.Dataset.from_tensor_slices(masks)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((images_dataset, masks_dataset))\n",
    "\n",
    "dataset = dataset.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.repeat(60)\n",
    "dataset = dataset.map(augmentate_images, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923",
   "metadata": {
    "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923"
   },
   "source": [
    "## Посмотрим на содержимое набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2",
   "metadata": {
    "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2"
   },
   "outputs": [],
   "source": [
    "images_and_masks = list(dataset.take(5))\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 5, figsize=(16, 6))\n",
    "\n",
    "for i, (image, masks) in enumerate(images_and_masks):\n",
    "    ax[0, i].set_title('Image')\n",
    "    ax[0, i].set_axis_off()\n",
    "    ax[0, i].imshow(image)\n",
    "        \n",
    "    ax[1, i].set_title('Mask')\n",
    "    ax[1, i].set_axis_off()    \n",
    "    ax[1, i].imshow(image/1.5)\n",
    "   \n",
    "    for channel in range(CLASSES):\n",
    "        contours = measure.find_contours(np.array(masks[:,:,channel]))\n",
    "        for contour in contours:\n",
    "            ax[1, i].plot(contour[:, 1], contour[:, 0], linewidth=1, color=COLORS[channel])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2c4bf-74e0-41de-86c6-e89687ef3ca4",
   "metadata": {
    "id": "a5a2c4bf-74e0-41de-86c6-e89687ef3ca4"
   },
   "source": [
    "## Разделим набор данных на обучающий и проверочный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e3f67-5f19-48f3-9a9f-09660b64d49a",
   "metadata": {
    "id": "cd9e3f67-5f19-48f3-9a9f-09660b64d49a"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(2000).cache()\n",
    "test_dataset = dataset.skip(2000).take(100).cache()\n",
    " \n",
    "train_dataset = train_dataset.batch(8)\n",
    "test_dataset = test_dataset.batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b3f41-5324-41e1-944d-809ec06ee959",
   "metadata": {
    "id": "482b3f41-5324-41e1-944d-809ec06ee959"
   },
   "source": [
    "## Обозначим основные блоки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b",
   "metadata": {
    "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b"
   },
   "outputs": [],
   "source": [
    "def input_layer():\n",
    "    return tf.keras.layers.Input(shape=SAMPLE_SIZE + (3,))\n",
    "\n",
    "def downsample_block(filters, size, batch_norm=True):\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    \n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if batch_norm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    return result\n",
    "\n",
    "def upsample_block(filters, size, dropout=False):\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    \n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                                        kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    if dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    return result\n",
    "\n",
    "def output_layer(size):\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "    return tf.keras.layers.Conv2DTranspose(CLASSES, size, strides=2, padding='same',\n",
    "                                           kernel_initializer=initializer, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc",
   "metadata": {
    "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc"
   },
   "source": [
    "## Построим U-NET подобную архитектуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40",
   "metadata": {
    "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40"
   },
   "outputs": [],
   "source": [
    "inp_layer = input_layer()\n",
    "\n",
    "downsample_stack = [\n",
    "    downsample_block(64, 4, batch_norm=False),\n",
    "    downsample_block(128, 4),\n",
    "    downsample_block(256, 4),\n",
    "    downsample_block(512, 4),\n",
    "    downsample_block(512, 4),\n",
    "    downsample_block(512, 4),\n",
    "    downsample_block(512, 4),\n",
    "]\n",
    "\n",
    "upsample_stack = [\n",
    "    upsample_block(512, 4, dropout=True),\n",
    "    upsample_block(512, 4, dropout=True),\n",
    "    upsample_block(512, 4, dropout=True),\n",
    "    upsample_block(256, 4),\n",
    "    upsample_block(128, 4),\n",
    "    upsample_block(64, 4)\n",
    "]\n",
    "\n",
    "out_layer = output_layer(4)\n",
    "\n",
    "# Реализуем skip connections\n",
    "x = inp_layer\n",
    "\n",
    "downsample_skips = []\n",
    "\n",
    "for block in downsample_stack:\n",
    "    x = block(x)\n",
    "    downsample_skips.append(x)\n",
    "    \n",
    "downsample_skips = reversed(downsample_skips[:-1])\n",
    "\n",
    "for up_block, down_block in zip(upsample_stack, downsample_skips):\n",
    "    x = up_block(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, down_block])\n",
    "\n",
    "out_layer = out_layer(x)\n",
    "\n",
    "unet_like = tf.keras.Model(inputs=inp_layer, outputs=out_layer)\n",
    "\n",
    "tf.keras.utils.plot_model(unet_like, show_shapes=True, dpi=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f732f-120b-49a8-bc7c-304709f12db5",
   "metadata": {
    "id": "b67f732f-120b-49a8-bc7c-304709f12db5"
   },
   "source": [
    "## Определим метрики и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381",
   "metadata": {
    "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381"
   },
   "outputs": [],
   "source": [
    "def dice_mc_metric(a, b):\n",
    "    a = tf.unstack(a, axis=3)\n",
    "    b = tf.unstack(b, axis=3)\n",
    "    \n",
    "    dice_summ = 0\n",
    "    \n",
    "    for i, (aa, bb) in enumerate(zip(a, b)):\n",
    "        numenator = 2 * tf.math.reduce_sum(aa * bb) + 1\n",
    "        denomerator = tf.math.reduce_sum(aa + bb) + 1\n",
    "        dice_summ += numenator / denomerator\n",
    "        \n",
    "    avg_dice = dice_summ / CLASSES\n",
    "    \n",
    "    return avg_dice\n",
    "\n",
    "def dice_mc_loss(a, b):\n",
    "    return 1 - dice_mc_metric(a, b)\n",
    "\n",
    "def dice_bce_mc_loss(a, b):\n",
    "    return 0.3 * dice_mc_loss(a, b) + tf.keras.losses.binary_crossentropy(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c",
   "metadata": {
    "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c"
   },
   "source": [
    "## Компилируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1",
   "metadata": {
    "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1"
   },
   "outputs": [],
   "source": [
    "unet_like.compile(optimizer='adam', loss=[dice_bce_mc_loss], metrics=[dice_mc_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfd329-31b4-4200-8282-1cb066d52b83",
   "metadata": {
    "id": "75bfd329-31b4-4200-8282-1cb066d52b83"
   },
   "source": [
    "## Обучаем нейронную сеть и сохраняем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb",
   "metadata": {
    "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb"
   },
   "outputs": [],
   "source": [
    "history_dice = unet_like.fit(train_dataset, validation_data=test_dataset, epochs=25, initial_epoch=0)\n",
    "\n",
    "unet_like.save_weights('SemanticSegmentationLesson/networks/unet_like')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70193172-cabb-44d0-b281-27951d1ad817",
   "metadata": {},
   "source": [
    "## Загружаем обученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c",
   "metadata": {
    "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c"
   },
   "outputs": [],
   "source": [
    "unet_like.load_weights('SemanticSegmentationLesson/networks/unet_like')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978c692-4136-419b-9365-e5fbf98bbf50",
   "metadata": {
    "id": "e978c692-4136-419b-9365-e5fbf98bbf50"
   },
   "source": [
    "## Проверим работу сети на всех кадрах из видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c",
   "metadata": {
    "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c"
   },
   "outputs": [],
   "source": [
    "rgb_colors = [\n",
    "    (0,   0,   0),\n",
    "    (255, 0,   0),\n",
    "    (0,   255, 0),\n",
    "    (0,   0,   255),\n",
    "    (255, 165, 0),\n",
    "    (255, 192, 203),\n",
    "    (0,   255, 255),\n",
    "    (255, 0,   255)\n",
    "]\n",
    "\n",
    "frames = sorted(glob.glob('SemanticSegmentationLesson/videos/original_video/*.jpg'))\n",
    "\n",
    "for filename in frames:\n",
    "    frame = imread(filename)\n",
    "    sample = resize(frame, SAMPLE_SIZE)\n",
    "    \n",
    "    predict = unet_like.predict(sample.reshape((1,) +  SAMPLE_SIZE + (3,)))\n",
    "    predict = predict.reshape(SAMPLE_SIZE + (CLASSES,))\n",
    "        \n",
    "    scale = frame.shape[0] / SAMPLE_SIZE[0], frame.shape[1] / SAMPLE_SIZE[1]\n",
    "    \n",
    "    frame = (frame / 1.5).astype(np.uint8)\n",
    "    \n",
    "    for channel in range(1, CLASSES): \n",
    "        contour_overlay = np.zeros((frame.shape[0], frame.shape[1]))\n",
    "        contours = measure.find_contours(np.array(predict[:,:,channel]))\n",
    "        \n",
    "        try:\n",
    "            for contour in contours:\n",
    "                rr, cc = polygon_perimeter(contour[:, 0] * scale[0],\n",
    "                                           contour[:, 1] * scale[1],\n",
    "                                           shape=contour_overlay.shape)\n",
    "                \n",
    "                contour_overlay[rr, cc] = 1        \n",
    "            \n",
    "            contour_overlay = dilation(contour_overlay, disk(1))\n",
    "            frame[contour_overlay == 1] = rgb_colors[channel]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    imsave(f'SemanticSegmentationLesson/videos/processed/{os.path.basename(filename)}', frame)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetwork.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
